{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53ff3671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "   Law RAG Console Application 시작\n",
      "============================================================\n",
      "FAISS 인덱스 로딩 중...\n",
      " 인덱스/메타데이터 로드 완료.\n",
      "LLM 모델 로딩 중...\n",
      "\n",
      "[치명적 오류] 모델 로딩 실패: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "INDEX_PATH = \"rag_index_retriever/faiss.index\"\n",
    "META_PATH  = \"rag_index/meta.pkl\"          \n",
    "SFT_DIR    = \"models/llama31-8b-sft-fold1\"        \n",
    "#EMB_MODEL  = \"nlpai-lab/KURE-v1\"\n",
    "EMB_MODEL = \"models/kure-law-retriever\"                     \n",
    "\n",
    "\n",
    "def load_index_and_meta():\n",
    "    import pickle\n",
    "    print(\"FAISS 인덱스 로딩 중...\") \n",
    "    if not os.path.exists(INDEX_PATH) or not os.path.exists(META_PATH):\n",
    "        print(f\"오류: 인덱스 파일을 찾을 수 없습니다. ({INDEX_PATH}, {META_PATH})\")\n",
    "        return None, None\n",
    "        \n",
    "    index = faiss.read_index(INDEX_PATH)\n",
    "    with open(META_PATH, \"rb\") as f:\n",
    "        meta = pickle.load(f)\n",
    "    print(\" 인덱스/메타데이터 로드 완료.\")\n",
    "    return index, meta\n",
    "\n",
    "def load_sft_model():\n",
    "    import os\n",
    "    from peft import PeftModel\n",
    "    from transformers import BitsAndBytesConfig\n",
    "    \n",
    "    print(\"LLM 모델 로딩 중...\") \n",
    "\n",
    "    HF_TOKEN = os.environ.get(\"HF_TOKEN\")  \n",
    "    BASE_MODEL = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "    ADAPTER_DIR = SFT_DIR  # 상단 상수 사용\n",
    "\n",
    "    bnb_cfg = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True, token=HF_TOKEN)\n",
    "    if tok.pad_token is None:\n",
    "        tok.pad_token = tok.eos_token\n",
    "\n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        quantization_config=bnb_cfg,\n",
    "        attn_implementation=\"sdpa\",\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16,\n",
    "        token=HF_TOKEN,\n",
    "    )\n",
    "\n",
    "    model = PeftModel.from_pretrained(base, ADAPTER_DIR)\n",
    "\n",
    "    print(\" LLM 모델 로드 완료.\")\n",
    "    return tok, model\n",
    "\n",
    "def load_embedder():\n",
    "    print(\"임베딩 모델 로딩 중...\") \n",
    "    return SentenceTransformer(EMB_MODEL) \n",
    "\n",
    "@torch.no_grad()\n",
    "def run_sft(tok, mdl, clause_text: str) -> str:\n",
    "    system = (\n",
    "        \"당신은 약관의 공정성을 분석하는 법률 전문가입니다.\\n\"\n",
    "        \"문맥상 주체 (고객/ 사업자) 를 명확히 구분하세요.\\n\"\n",
    "        \"반드시 아래 한 줄 포맷만 출력하세요:\\n\"\n",
    "        \"분야: <정수> / 불공정여부: <유리|불리> / 근거: <간결한 문장 또는 '해당 없음'>\"\n",
    "    )\n",
    "    user = f\"다음 약관 조항의 문맥을 이해하여 분야 분류, 불공정 여부 판단, 판단 근거를 요약하시오.\\n\\n입력:\\n{clause_text}\"\n",
    "\n",
    "    chat = [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": user}\n",
    "    ]\n",
    "    prompt = tok.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "    inputs = tok(prompt, return_tensors=\"pt\").to(mdl.device)\n",
    "    out_ids = mdl.generate(\n",
    "        **inputs, \n",
    "        max_new_tokens=256, \n",
    "        do_sample=False\n",
    "    )\n",
    "    out_txt = tok.decode(out_ids[0], skip_special_tokens=True)\n",
    "    ans = out_txt.split(\"assistant\\n\")[-1].strip()\n",
    "    return ans\n",
    "\n",
    "def parse_reason(answer_line: str) -> str:\n",
    "    parts = [p.strip() for p in answer_line.split(\"/\") if p.strip()]\n",
    "    reason = \"\"\n",
    "    for p in parts:\n",
    "        if p.startswith(\"근거:\"):\n",
    "            reason = p.replace(\"근거:\", \"\").strip()\n",
    "            break\n",
    "    return reason\n",
    "\n",
    "def embed(embedder, texts):\n",
    "    embs = embedder.encode(texts, normalize_embeddings=True)\n",
    "    return np.asarray(embs, dtype=\"float32\")\n",
    "\n",
    "def search(index, query_vec, topk=5):\n",
    "    D, I = index.search(query_vec, topk)\n",
    "    return I[0], D[0]\n",
    "\n",
    "def build_report(clause_text, sft_answer, meta, hits=None, similarities=None):\n",
    "    answer_str = sft_answer.strip()\n",
    "    is_unfair = \"불공정여부: 불리\" in answer_str\n",
    "\n",
    "    report = {\n",
    "        \"input_clause\": clause_text,\n",
    "        \"llm_output\": answer_str,\n",
    "    }\n",
    "\n",
    "    laws = []\n",
    "    if is_unfair and hits is not None and len(hits) > 0:\n",
    "        if similarities is not None and len(similarities) == len(hits):\n",
    "            hit_data = zip(hits, similarities)\n",
    "        else:\n",
    "            hit_data = zip(hits, [None] * len(hits))\n",
    "\n",
    "        for idx, sim in hit_data:\n",
    "            if idx < 0 or idx >= len(meta): continue \n",
    "            \n",
    "            rec = meta[int(idx)]\n",
    "            law_entry = {\n",
    "                \"clauseField\": rec.get(\"clauseField\"),\n",
    "                \"law_text\": rec.get(\"law_text\"),\n",
    "                \"similarity\": float(sim) if sim is not None else None\n",
    "            }\n",
    "            laws.append(law_entry)\n",
    "            \n",
    "    report[\"retrieved_laws\"] = laws\n",
    "    return report\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"===\" * 20)\n",
    "    print(\"   Law RAG Console Application 시작\")\n",
    "    print(\"===\" * 20)\n",
    "\n",
    "    #  모델 및 데이터 로드\n",
    "    try:\n",
    "        index, meta = load_index_and_meta()\n",
    "        if index is None: return\n",
    "\n",
    "        tok, mdl = load_sft_model()\n",
    "        embedder = load_embedder()\n",
    "        print(\"\\n모든 모델 로드 완료\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[치명적 오류] 모델 로딩 실패: {e}\")\n",
    "        return\n",
    "\n",
    "    #  사용자 입력 루프\n",
    "    while True:\n",
    "        print(\"-\" * 60)\n",
    "        clause = input(\"분석할 약관 조항을 입력하세요 (종료하려면 'q' 또는 'exit' 입력):\\n>> \")\n",
    "        \n",
    "        if clause.lower() in ['q', 'exit', 'quit']:\n",
    "            print(\"프로그램을 종료합니다.\")\n",
    "            break\n",
    "        \n",
    "        if not clause.strip():\n",
    "            print(\"! 내용을 입력해주세요.\")\n",
    "            continue\n",
    "\n",
    "        print(\"\\n... 분석 중입니다 ...\\n\")\n",
    "        \n",
    "        try:\n",
    "            # SFT 추론\n",
    "            answer = run_sft(tok, mdl, clause)\n",
    "            reason = parse_reason(answer)\n",
    "\n",
    "            # 불공정(불리) 판정 시에만 RAG 검색 수행\n",
    "            if \"불공정여부: 유리\" in answer:\n",
    "                report = build_report(clause, answer, meta, hits=None, similarities=None)\n",
    "            else:\n",
    "                fused_query = f\"{clause}\\n\\n판단근거: {reason}\" if reason else clause\n",
    "                qv = embed(embedder, [fused_query])\n",
    "                ids, similarities = search(index, qv, topk=5)\n",
    "                report = build_report(clause, answer, meta, hits=ids, similarities=similarities)\n",
    "\n",
    "            # 결과 출력 \n",
    "            print(\"\\n[분석 결과]\")\n",
    "            print(json.dumps(report, indent=2, ensure_ascii=False))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"분석 중 오류 발생: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f85ce51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ally010314/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'transformers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtrl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SFTTrainer, SFTConfig\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtransformers\u001b[49m\u001b[38;5;241m.\u001b[39mlogging\u001b[38;5;241m.\u001b[39mset_verbosity_error()\n\u001b[1;32m     11\u001b[0m MY_TOKEN \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHF_TOKEN\u001b[39m\u001b[38;5;124m\"\u001b[39m)            \n\u001b[1;32m     12\u001b[0m BASE_MODEL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Meta-Llama-3.1-8B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'transformers' is not defined"
     ]
    }
   ],
   "source": [
    "import torch, os, json, pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "MY_TOKEN = os.environ.get(\"HF_TOKEN\")            \n",
    "BASE_MODEL = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "MAX_SEQ_LEN = 512\n",
    "OUTPUT_DIR = \"models/llama31-8b-sft-fold10\"\n",
    "\n",
    "os.environ[\"TORCH_COMPILE_DISABLE\"] = \"1\"\n",
    "\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,    \n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "print(f\"'{BASE_MODEL}' 로드 중 ...\")\n",
    "tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True, token=MY_TOKEN)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "mdl = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=bnb_cfg,\n",
    "    attn_implementation=\"sdpa\",             \n",
    "    device_map=\"auto\",\n",
    "    dtype=torch.float16,\n",
    "    token=MY_TOKEN,\n",
    ")\n",
    "print(\"--- 모델 로딩 완료 ---\")\n",
    "\n",
    "mdl = prepare_model_for_kbit_training(mdl)\n",
    "\n",
    "lora_cfg = LoraConfig(\n",
    "    r=8, lora_alpha=16, lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\n",
    "                    \"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
    "    bias=\"none\", task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "mdl = get_peft_model(mdl, lora_cfg)\n",
    "\n",
    "TRAIN_PATH = \"data/kfold_data/train_fold_10.jsonl\"\n",
    "VAL_PATH   = \"data/kfold_data/val_fold_10.jsonl\"\n",
    "\n",
    "def load_jsonl(path):\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        return [json.loads(l) for l in f if l.strip()]\n",
    "\n",
    "train_rows = load_jsonl(TRAIN_PATH)\n",
    "val_rows = load_jsonl(VAL_PATH)\n",
    "\n",
    "train_df = pd.DataFrame(train_rows)\n",
    "val_df = pd.DataFrame(val_rows)\n",
    "\n",
    "print(f\"Train 샘플 수: {len(train_df)}, Val 샘플 수: {len(val_df)}\")\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"당신은 약관의 공정성을 분석하는 법률 전문가입니다.\\n\"\n",
    "    \"문맥상 주체 (고객/ 사업자) 를 명확히 구분하세요.\\n\"\n",
    "    \"반드시 아래 한 줄 포맷만 출력하세요:\\n\"\n",
    "    \"분야: <정수> / 불공정여부: <유리|불리> / 근거: <간결한 문장 또는 '해당 없음'>\"\n",
    ")\n",
    "\n",
    "def to_messages(r):\n",
    "    inst, inp, out = r.get(\"instruction\",\"\"), r.get(\"input\",\"\"), r.get(\"output\",\"\")\n",
    "    user_text = inst if not inp else f\"{inst}\\n\\n입력:\\n{inp}\"\n",
    "    return [\n",
    "        {\"role\":\"system\",\"content\":SYSTEM_PROMPT},\n",
    "        {\"role\":\"user\",\"content\":user_text},\n",
    "        {\"role\":\"assistant\",\"content\":out.strip()},\n",
    "    ]\n",
    "def format_example(ex):\n",
    "    text = tok.apply_chat_template(\n",
    "        to_messages(ex), tokenize=False, add_generation_prompt=False\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "train_ds = Dataset.from_pandas(train_df)\n",
    "train_ds = train_ds.map(format_example, remove_columns=list(train_df.columns))\n",
    "\n",
    "val_ds = Dataset.from_pandas(val_df)\n",
    "val_ds = val_ds.map(format_example, remove_columns=list(val_df.columns))\n",
    "\n",
    "\n",
    "print(f\"데이터셋: train {len(train_ds)}, val {len(val_ds)}\")\n",
    "\n",
    "sft_cfg = SFTConfig(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",             \n",
    "    save_strategy=\"epoch\",             \n",
    "    save_total_limit=1,\n",
    "    max_grad_norm=0.3,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",\n",
    "    fp16=True, bf16=False,\n",
    "    dataloader_num_workers=0,          \n",
    "    dataloader_pin_memory=False,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_length=MAX_SEQ_LEN,\n",
    "    packing=False,\n",
    "    group_by_length=True,               \n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=mdl,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    args=sft_cfg,\n",
    "    processing_class=tok,\n",
    ")\n",
    "\n",
    "print(\"--- 파인튜닝 시작 ---\")\n",
    "trainer.train()\n",
    "print(\"--- 파인튜닝 완료 ---\")\n",
    "\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tok.save_pretrained(OUTPUT_DIR)\n",
    "print(f\"저장 완료: {OUTPUT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37ac9f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ally010314/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> 전체 평가 프로세스 시작...\n",
      "Llama 3.1 Base 4bit 로딩 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:28<00:00,  7.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "####################################################\n",
      "### 평가 대상: models/llama31-8b-sft-fold10\n",
      "### 검증 파일: data/kfold_data/val_fold_10.jsonl\n",
      "####################################################\n",
      "\n",
      "[1/2] Base Model 평가 시작...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Base 평가 중: 100%|██████████| 900/900 [1:03:31<00:00,  4.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[2/2] SFT Model 평가 시작...\n",
      "[SFT] 어댑터 로딩 중... models/llama31-8b-sft-fold10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SFT 평가 중: 100%|██████████| 900/900 [1:15:25<00:00,  5.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== 최종 결과 비교: models/llama31-8b-sft-fold10 ======\n",
      "[불공정여부 (불리=Positive)]\n",
      "Accuracy : Base=0.7022  -> SFT=0.9789\n",
      "Precision: Base=0.5568  -> SFT=0.9891\n",
      "Recall   : Base=0.3403  -> SFT=0.9444\n",
      "F1 Score : Base=0.4224  -> SFT=0.9663\n",
      "\n",
      "[분야 분류]\n",
      "Accuracy : Base=0.3100  -> SFT=0.4322\n",
      "Macro F1 : Base=0.2812  -> SFT=0.4507\n",
      "===================================================\n",
      "\n",
      ">>> 평가 종료. 메모리 정리 중...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "import gc\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 로깅 설정\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "# =============================================================================\n",
    "# 1. 설정 및 상수\n",
    "# =============================================================================\n",
    "\n",
    "# 평가할 모델과 데이터 입력 (필요에 따라 수정하세요)\n",
    "EVAL_TARGETS = [\n",
    "    (\"models/llama31-8b-sft-fold10\", \"data/kfold_data/val_fold_10.jsonl\"),\n",
    "]\n",
    "\n",
    "BASE_MODEL = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "\n",
    "CATEGORY_MAP = {\n",
    "    \"1\": \"가맹계약\", \"2\": \"공급계약\", \"3\": \"분양계약\", \"4\": \"신탁계약\",\n",
    "    \"5\": \"임대차계약\", \"6\": \"입소, 입주, 입점계약\", \"7\": \"신용카드\", \"8\": \"은행여신\",\n",
    "    \"9\": \"은행전자금융서비스\", \"10\": \"전자결제수단\", \"11\": \"전자금융거래\",\n",
    "    \"12\": \"상해보험\", \"13\": \"손해보험\", \"14\": \"질병보험\", \"15\": \"연금보험\",\n",
    "    \"16\": \"자동차보험\", \"17\": \"책임보험\", \"18\": \"화재보험\", \"19\": \"증권사1\",\n",
    "    \"20\": \"증권사2\", \"21\": \"증권사3\", \"22\": \"여객운송\", \"23\": \"화물운송\",\n",
    "    \"24\": \"개인정보취급방침\", \"25\": \"게임\", \"26\": \"국내·외 여행\",\n",
    "    \"27\": \"결혼정보서비스\", \"28\": \"렌트(자동차 이외)\", \"29\": \"마일리지/포인트\",\n",
    "    \"30\": \"보증\", \"31\": \"사이버몰\", \"32\": \"산후조리원\", \"33\": \"상조서비스\",\n",
    "    \"34\": \"상품권\", \"35\": \"생명보험\", \"36\": \"예식업\", \"37\": \"온라인서비스\",\n",
    "    \"38\": \"자동차 리스 및 렌트\", \"39\": \"체육시설\", \"40\": \"택배\",\n",
    "    \"41\": \"통신, 방송서비스\", \"42\": \"교육\", \"43\": \"매매계약\"\n",
    "}\n",
    "\n",
    "CATEGORY_HINT_TEXT = \"\\n\".join([f\"{k}: {v}\" for k, v in CATEGORY_MAP.items()])\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 2. 유틸리티 함수\n",
    "# =============================================================================\n",
    "\n",
    "def parse_output_line(line: str):\n",
    "    field_id = None\n",
    "    unfair = None\n",
    "    parts = [p.strip() for p in line.split(\"/\")]\n",
    "    \n",
    "    # 1. 분야 파싱\n",
    "    head1 = parts[0] if len(parts) >= 1 else line\n",
    "    m = re.search(r\"(\\d+)\", head1)\n",
    "    if m:\n",
    "        try: field_id = int(m.group(1))\n",
    "        except ValueError: field_id = None\n",
    "            \n",
    "    for k, v in CATEGORY_MAP.items():\n",
    "        if v in head1:\n",
    "            try: \n",
    "                field_id = int(k)\n",
    "                break\n",
    "            except ValueError: continue\n",
    "\n",
    "    # 2. 불공정여부 파싱\n",
    "    if len(parts) >= 2: head2 = \" / \".join(parts[:2])\n",
    "    else: head2 = head1\n",
    "\n",
    "    if \"불리\" in head2: unfair = 1\n",
    "    elif \"유리\" in head2: unfair = 0\n",
    "\n",
    "    return field_id, unfair\n",
    "\n",
    "\n",
    "def build_prompt(tok, clause: str, with_category_hint: bool):\n",
    "    system = (\n",
    "        \"당신은 약관의 공정성을 분석하는 법률 전문가입니다.\\n\"\n",
    "        \"문맥상 주체 (고객/사업자)를 명확히 구분하세요.\\n\"\n",
    "        \"반드시 아래 한 줄 포맷만 출력하세요:\\n\"\n",
    "        \"분야: <정수> / 불공정여부: <유리|불리> / 근거: <간결한 문장 또는 '해당 없음'>\"\n",
    "    )\n",
    "    if with_category_hint:\n",
    "        system += (\n",
    "            \"\\n\\n[분야 번호 안내]\\n\"\n",
    "            \"분야는 반드시 아래 번호 중 하나여야 합니다.\\n\"\n",
    "            f\"{CATEGORY_HINT_TEXT}\\n\"\n",
    "        )\n",
    "    user = (\n",
    "        \"다음 약관 조항의 문맥을 이해하여 분야 분류, 불공정 여부 판단, \"\n",
    "        \"판단 근거를 요약하세요.\\n\\n입력:\\n\" + clause\n",
    "    )\n",
    "    chat = [\n",
    "        {\"role\": \"system\", \"content\": system},\n",
    "        {\"role\": \"user\", \"content\": user},\n",
    "    ]\n",
    "    return tok.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "\n",
    "def extract_answer_text(raw_txt: str) -> str:\n",
    "    if \"assistant\\n\" in raw_txt:\n",
    "        return raw_txt.split(\"assistant\\n\")[-1].strip()\n",
    "    return raw_txt.strip()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 3. 모델 로딩 및 추론 함수\n",
    "# =============================================================================\n",
    "\n",
    "def load_llama_base_4bit():\n",
    "    print(\"Llama 3.1 Base 4bit 로딩 중...\")\n",
    "    HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "    \n",
    "    bnb_cfg = BitsAndBytesConfig(\n",
    "        load_in_4bit=True, \n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16, \n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    \n",
    "    tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True, token=HF_TOKEN)\n",
    "    if tok.pad_token is None: tok.pad_token = tok.eos_token\n",
    "    \n",
    "    base = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL, \n",
    "        quantization_config=bnb_cfg, \n",
    "        attn_implementation=\"sdpa\",\n",
    "        device_map=\"auto\", \n",
    "        torch_dtype=torch.float16, \n",
    "        token=HF_TOKEN,\n",
    "    )\n",
    "    base.eval()\n",
    "    return tok, base\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_model(tok, model, clause: str, with_category_hint: bool) -> str:\n",
    "    prompt = build_prompt(tok, clause, with_category_hint=with_category_hint)\n",
    "    inputs = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    out_ids = model.generate(\n",
    "        **inputs, max_new_tokens=256, do_sample=False,\n",
    "    )\n",
    "    return tok.decode(out_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def evaluate_model(tok, model, data, tag: str, with_category_hint: bool, debug: bool = False):\n",
    "    y_true_unfair, y_pred_unfair = [], []\n",
    "    y_true_field, y_pred_field = [], []\n",
    "\n",
    "    pbar = tqdm(data, desc=f\"{tag} 평가 중\")\n",
    "    for idx, ex in enumerate(pbar, start=1):\n",
    "        clause = ex[\"input\"]\n",
    "        gold_out = ex[\"output\"]\n",
    "\n",
    "        g_field, g_unfair = parse_output_line(gold_out)\n",
    "        if g_unfair is None: continue\n",
    "\n",
    "        raw_txt = run_model(tok, model, clause, with_category_hint=with_category_hint)\n",
    "        pred_line = extract_answer_text(raw_txt)\n",
    "        p_field, p_unfair = parse_output_line(pred_line)\n",
    "\n",
    "        if debug:\n",
    "            tqdm.write(f\"\\n====== [{tag}] Sample {idx} ======\")\n",
    "            tqdm.write(f\"GOLD: {gold_out}\")\n",
    "            tqdm.write(f\"PRED: {pred_line}\")\n",
    "            tqdm.write(f\" -> P_Field: {p_field}, P_Unfair: {p_unfair}\")\n",
    "\n",
    "        # 불공정 여부 후처리 (None인 경우 반대로 예측 처리 or 0 처리, 여기선 기존 로직 유지)\n",
    "        if p_unfair is None:\n",
    "            p_unfair = 1 - g_unfair \n",
    "\n",
    "        y_true_unfair.append(g_unfair)\n",
    "        y_pred_unfair.append(p_unfair)\n",
    "\n",
    "        if g_field is not None:\n",
    "            y_true_field.append(g_field)\n",
    "            y_pred_field.append(-1 if p_field is None else p_field)\n",
    "\n",
    "    # 지표 계산\n",
    "    unfair_acc = accuracy_score(y_true_unfair, y_pred_unfair)\n",
    "    unfair_prec, unfair_rec, unfair_f1, _ = precision_recall_fscore_support(\n",
    "        y_true_unfair, y_pred_unfair, average=\"binary\", pos_label=1,\n",
    "    )\n",
    "    field_acc = accuracy_score(y_true_field, y_pred_field)\n",
    "    field_prec, field_rec, field_f1, _ = precision_recall_fscore_support(\n",
    "        y_true_field, y_pred_field, average=\"macro\", zero_division=0,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"unfair_acc\": unfair_acc, \"unfair_prec\": unfair_prec,\n",
    "        \"unfair_rec\": unfair_rec, \"unfair_f1\": unfair_f1,\n",
    "        \"field_acc\": field_acc, \"field_f1\": field_f1,\n",
    "    }\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 4. 메인 실행 함수\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    if not EVAL_TARGETS:\n",
    "        return\n",
    "\n",
    "    # [수정됨] Base 모델을 루프 밖에서 한 번만 로드하여 GPU 메모리 문제 방지\n",
    "    print(\">>> 전체 평가 프로세스 시작...\")\n",
    "    tok, base_model = load_llama_base_4bit()\n",
    "\n",
    "    try:\n",
    "        for sft_path, val_path in EVAL_TARGETS:\n",
    "            print(f\"\\n####################################################\")\n",
    "            print(f\"### 평가 대상: {sft_path}\")\n",
    "            print(f\"### 검증 파일: {val_path}\")\n",
    "            print(f\"####################################################\")\n",
    "\n",
    "            if not os.path.exists(sft_path) or not os.path.exists(val_path):\n",
    "                print(f\" 경로가 존재하지 않습니다. 건너뜁니다.\")\n",
    "                continue\n",
    "\n",
    "            with open(val_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "            # -------------------------------------------------------\n",
    "            # [1/2] Base Model 평가 (순정 상태)\n",
    "            # -------------------------------------------------------\n",
    "            print(\"\\n[1/2] Base Model 평가 시작...\")\n",
    "            \n",
    "            # 혹시 이전 루프에서 어댑터가 남아있다면 제거 (안전장치)\n",
    "            if hasattr(base_model, \"disable_adapter_layers\"):\n",
    "                 base_model.disable_adapter_layers()\n",
    "            \n",
    "            base_res = evaluate_model(\n",
    "                tok, base_model, data, tag=\"Base\", with_category_hint=True, debug=False\n",
    "            )\n",
    "\n",
    "            # -------------------------------------------------------\n",
    "            # [2/2] SFT Model 평가 (어댑터 장착)\n",
    "            # -------------------------------------------------------\n",
    "            print(\"\\n[2/2] SFT Model 평가 시작...\")\n",
    "            print(f\"[SFT] 어댑터 로딩 중... {sft_path}\")\n",
    "            \n",
    "            # [핵심 수정] 기존 base_model 위에 어댑터만 씌움 (메모리 재사용)\n",
    "            sft_model = PeftModel.from_pretrained(base_model, sft_path)\n",
    "            sft_model.eval()\n",
    "\n",
    "            sft_res = evaluate_model(\n",
    "                tok, sft_model, data, tag=\"SFT\", with_category_hint=True, debug=False\n",
    "            )\n",
    "\n",
    "            # 결과 출력\n",
    "            print(f\"\\n====== 최종 결과 비교: {sft_path} ======\")\n",
    "            print(\"[불공정여부 (불리=Positive)]\")\n",
    "            print(f\"Accuracy : Base={base_res['unfair_acc']:.4f}  -> SFT={sft_res['unfair_acc']:.4f}\")\n",
    "            print(f\"Precision: Base={base_res['unfair_prec']:.4f}  -> SFT={sft_res['unfair_prec']:.4f}\")\n",
    "            print(f\"Recall   : Base={base_res['unfair_rec']:.4f}  -> SFT={sft_res['unfair_rec']:.4f}\")\n",
    "            print(f\"F1 Score : Base={base_res['unfair_f1']:.4f}  -> SFT={sft_res['unfair_f1']:.4f}\")\n",
    "\n",
    "            print(\"\\n[분야 분류]\")\n",
    "            print(f\"Accuracy : Base={base_res['field_acc']:.4f}  -> SFT={sft_res['field_acc']:.4f}\")\n",
    "            print(f\"Macro F1 : Base={base_res['field_f1']:.4f}  -> SFT={sft_res['field_f1']:.4f}\")\n",
    "            print(\"===================================================\\n\")\n",
    "            \n",
    "            # [중요] 다음 루프나 종료를 위해 어댑터 연결 해제 (메모리 정리)\n",
    "            # del sft_model 만으로는 부족할 수 있으므로 명시적 제거 시도\n",
    "            del sft_model\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # base_model은 다음 루프를 위해 유지됩니다.\n",
    "\n",
    "    finally:\n",
    "        # 프로그램 완전 종료 시 정리\n",
    "        print(\">>> 평가 종료. 메모리 정리 중...\")\n",
    "        del base_model\n",
    "        del tok\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
