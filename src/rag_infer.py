# rag_infer.py
import argparse, json, os
import faiss, numpy as np
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer

# ==== 0) ê²½ë¡œë“¤ ====
INDEX_PATH = "nlp_project/rag_index/faiss.index"
META_PATH  = "nlp_project/rag_index/meta.pkl"          # id -> {law_text, clauseField, ...}
SFT_DIR    = "nlp_project/models/llama31-8b-sft-fold2"              # ë„ˆê°€ ì €ì¥í•œ SFT ì²´í¬í¬ì¸íŠ¸
EMB_MODEL  = "nlpai-lab/KURE-v1"                  # ì œì•ˆì„œ ì§€ì • ì„ë² ë”© ëª¨ë¸

# ==== 1) ë¡œë“œ ====
def load_index_and_meta():
    import pickle
    index = faiss.read_index(INDEX_PATH)
    with open(META_PATH, "rb") as f:
        meta = pickle.load(f)
    # meta: list of dicts with keys: id, law_text, clauseField, file_name, ...
    return index, meta

def load_sft_model():
    import os
    from peft import PeftModel
    from transformers import BitsAndBytesConfig

    HF_TOKEN = os.environ.get("HF_TOKEN")  # ë˜ëŠ” ë¬¸ìì—´ë¡œ ì§ì ‘ ë„£ì–´ë„ ë¨
    BASE_MODEL = "meta-llama/Meta-Llama-3.1-8B-Instruct"
    ADAPTER_DIR = "nlp_project/models/llama31-8b-sft-fold2"  # ë„ˆì˜ LoRA ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ

    bnb_cfg = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )

    # 1) í† í¬ë‚˜ì´ì €
    tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True, token=HF_TOKEN)
    if tok.pad_token is None:
        tok.pad_token = tok.eos_token

    # 2) ë² ì´ìŠ¤(4bit, SDPA)  â† í•™ìŠµ ì„¤ì •ê³¼ ì¼ì¹˜ì‹œí‚´
    base = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        quantization_config=bnb_cfg,
        attn_implementation="sdpa",
        device_map="auto",
        torch_dtype=torch.float16,
        token=HF_TOKEN,
    )

    # 3) LoRA ì–´ëŒ‘í„° ë¡œë“œ (í•™ìŠµ ë‹¹ì‹œ ê²½ë¡œ ì²´ê³„ì™€ ë™ì¼í•˜ê²Œ ë§ì¶°ì§)
    model = PeftModel.from_pretrained(base, ADAPTER_DIR)

    # 4) 4bitì—ì„œëŠ” merge ë¶ˆê°€(ê¶Œì¥ ì•ˆë¨). ê·¸ëƒ¥ adapter ë¶™ì¸ ì±„ë¡œ ì‚¬ìš©.
    # try:
    #     model = model.merge_and_unload()  # 4bitë©´ ì—ëŸ¬/ì˜ë¯¸ì—†ìŒ
    # except Exception:
    #     pass

    return tok, model



def load_embedder():
    return SentenceTransformer(EMB_MODEL)  # KURE-v1

# ==== 2) SFT í•œì¤„ ì¶”ë¡  ====
@torch.no_grad()
def run_sft(tok, mdl, clause_text: str) -> str:
    system = (
        "ë‹¹ì‹ ì€ ì•½ê´€ì˜ ê³µì •ì„±ì„ ë¶„ì„í•˜ëŠ” ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n"
        "ë¬¸ë§¥ìƒ ì£¼ì²´ (ê³ ê°/ ì‚¬ì—…ì) ë¥¼ ëª…í™•íˆ êµ¬ë¶„í•˜ì„¸ìš”.\n"
        "ë°˜ë“œì‹œ ì•„ë˜ í•œ ì¤„ í¬ë§·ë§Œ ì¶œë ¥í•˜ì„¸ìš”:\n"
        "ë¶„ì•¼: <ì •ìˆ˜> / ë¶ˆê³µì •ì—¬ë¶€: <ìœ ë¦¬|ë¶ˆë¦¬> / ê·¼ê±°: <ê°„ê²°í•œ ë¬¸ì¥ ë˜ëŠ” 'í•´ë‹¹ ì—†ìŒ'>"
    )
    user = f"ë‹¤ìŒ ì•½ê´€ ì¡°í•­ì˜ ë¬¸ë§¥ì„ ì´í•´í•˜ì—¬ ë¶„ì•¼ ë¶„ë¥˜, ë¶ˆê³µì • ì—¬ë¶€ íŒë‹¨, íŒë‹¨ ê·¼ê±°ë¥¼ ìš”ì•½í•˜ì‹œì˜¤.\n\nì…ë ¥:\n{clause_text}"

    chat = [
        {"role": "system", "content": system},
        {"role": "user", "content": user}
    ]
    prompt = tok.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)
    inputs = tok(prompt, return_tensors="pt").to(mdl.device)
    out_ids = mdl.generate(
        **inputs, 
        max_new_tokens=256, 
        do_sample=False
    )
    out_txt = tok.decode(out_ids[0], skip_special_tokens=True)
    # ë§ˆì§€ë§‰ assistantë§Œ ì¶”ì¶œ
    ans = out_txt.split("assistant\n")[-1].strip()
    return ans

def parse_reason(answer_line: str) -> str:
    # "ë¶„ì•¼: X / ë¶ˆê³µì •ì—¬ë¶€: ìœ ë¦¬|ë¶ˆë¦¬ / ê·¼ê±°: Y" ì—ì„œ ê·¼ê±°ë§Œ ì¶”ì¶œ
    parts = [p.strip() for p in answer_line.split("/") if p.strip()]
    reason = ""
    for p in parts:
        if p.startswith("ê·¼ê±°:"):
            reason = p.replace("ê·¼ê±°:", "").strip()
            break
    return reason

# ==== 3) ê²€ìƒ‰ ====
def embed(embedder, texts):
    embs = embedder.encode(texts, normalize_embeddings=True)
    return np.asarray(embs, dtype="float32")

def search(index, query_vec, topk=5):
    D, I = index.search(query_vec, topk)
    return I[0], D[0]

# ==== 4) ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„± ====
def build_report(clause_text, sft_answer, meta, hits=None):
    answer_str = sft_answer.strip()
    is_unfair = "ë¶ˆê³µì •ì—¬ë¶€: ë¶ˆë¦¬" in answer_str

    report = {
        "input_clause": clause_text,
        "llm_output": answer_str,
    }

    laws = []
    if is_unfair and hits is not None and len(hits) > 0:
        for idx in hits:
            rec = meta[int(idx)]
            laws.append({
                "clauseField": rec.get("clauseField"),
                "law_text": rec.get("law_text")
            })
    report["retrieved_laws"] = laws
    return report



def main():
    index, meta = load_index_and_meta()
    tok, mdl = load_sft_model()
    embedder = load_embedder()

    print("âœ… ëª¨ë¸ ë° ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ.")
    print("ì—”í„°ë§Œ ëˆ„ë¥´ë©´ ì¢…ë£Œë©ë‹ˆë‹¤.\n")

    while True:
        clause = input("ğŸ” ì•½ê´€ ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”:\n> ").strip()
        if not clause:
            print("\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # 1) LLM ì¶”ë¡ 
        answer = run_sft(tok, mdl, clause)
        reason = parse_reason(answer)

        # 2) ìœ ë¦¬í•œ ê²½ìš° ê²€ìƒ‰ ìŠ¤í‚µ
        if "ë¶ˆê³µì •ì—¬ë¶€: ìœ ë¦¬" in answer:
            report = build_report(clause, answer, meta, hits=None)
        else:
            # ë¶ˆë¦¬í•œ ê²½ìš°ë§Œ ê·¼ê±° + ì›ë¬¸ìœ¼ë¡œ ê²€ìƒ‰
            fused_query = f"{clause}\n\níŒë‹¨ê·¼ê±°: {reason}" if reason else clause
            qv = embed(embedder, [fused_query])
            ids, _ = search(index, qv, topk=5)
            report = build_report(clause, answer, meta, hits=ids)

        print(json.dumps(report, ensure_ascii=False, indent=2))
        print("\n" + "="*80 + "\n")

if __name__ == "__main__":
    main()
