# rag_infer.py
import os
import faiss, numpy as np
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer

BASE_DIR = os.path.dirname(os.path.abspath(__file__))      # .../nlp_project/src
PROJECT_DIR = os.path.abspath(os.path.join(BASE_DIR, ".."))# .../nlp_project

INDEX_PATH = os.path.join(PROJECT_DIR, "rag_index_retriever", "faiss.index")
META_PATH  = os.path.join(PROJECT_DIR, "rag_index_retriever", "meta.pkl")

EMB_MODEL  = os.path.join(PROJECT_DIR, "models", "kure-law-retriever", "checkpoint-94")



# ì œì•ˆì„œ ì§€ì • ì„ë² ë”© ëª¨ë¸

# ==== 1) ë¡œë“œ ====
def load_index_and_meta():
    import pickle
    index = faiss.read_index(INDEX_PATH)
    with open(META_PATH, "rb") as f:
        meta = pickle.load(f)
    # meta: list of dicts with keys: id, law_text, clauseField, file_name, ...
    return index, meta
from peft import PeftModel
from transformers import BitsAndBytesConfig

def load_sft_model():
    HF_TOKEN = os.environ.get("HF_TOKEN")
    BASE_MODEL = "meta-llama/Meta-Llama-3.1-8B-Instruct"
    ADAPTER_DIR = SFT_DIR   # ìœ„ì—ì„œ ë§Œë“  ì ˆëŒ€ê²½ë¡œ ê·¸ëŒ€ë¡œ ì‚¬ìš©

    print(f"SFT(LLM) ëª¨ë¸ ë¡œë”© ì¤‘... ({ADAPTER_DIR})")

    # 1) í† í¬ë‚˜ì´ì €
    tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True, token=HF_TOKEN)
    if tok.pad_token is None:
        tok.pad_token = tok.eos_token

    # 2) 4bit base
    bnb_cfg = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
    )

    base = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        quantization_config=bnb_cfg,
        attn_implementation="sdpa",
        device_map="auto",
        torch_dtype=torch.float16,
        token=HF_TOKEN,
    )

    # 3) ì–´ëŒ‘í„° ê²½ë¡œ ì²´í¬ (ë””ë²„ê¹…ìš©)
    config_path = os.path.join(ADAPTER_DIR, "adapter_config.json")
    if not os.path.exists(config_path):
        raise FileNotFoundError(f"adapter_config.json ì´ ì—¬ê¸° ì—†ìŒ: {config_path}")

    model = PeftModel.from_pretrained(base, ADAPTER_DIR)
    model.eval()

    print("âœ… SFT(LLM) ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")
    return tok, model




def load_embedder():
    return SentenceTransformer(EMB_MODEL)  # KURE-v1

# ==== 2) SFT í•œì¤„ ì¶”ë¡  ====
@torch.no_grad()
def run_sft(tok, mdl, clause_text: str) -> str:
    system = (
        "ë‹¹ì‹ ì€ ì•½ê´€ì˜ ê³µì •ì„±ì„ ë¶„ì„í•˜ëŠ” ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n"
        "ë¬¸ë§¥ìƒ ì£¼ì²´ (ê³ ê°/ ì‚¬ì—…ì) ë¥¼ ëª…í™•íˆ êµ¬ë¶„í•˜ì„¸ìš”.\n"
        "ë°˜ë“œì‹œ ì•„ë˜ í•œ ì¤„ í¬ë§·ë§Œ ì¶œë ¥í•˜ì„¸ìš”:\n"
        "ë¶„ì•¼: <ì •ìˆ˜> / ë¶ˆê³µì •ì—¬ë¶€: <ìœ ë¦¬|ë¶ˆë¦¬> / ê·¼ê±°: <ê°„ê²°í•œ ë¬¸ì¥ ë˜ëŠ” 'í•´ë‹¹ ì—†ìŒ'>"
    )
    user = f"ë‹¤ìŒ ì•½ê´€ ì¡°í•­ì˜ ë¬¸ë§¥ì„ ì´í•´í•˜ì—¬ ë¶„ì•¼ ë¶„ë¥˜, ë¶ˆê³µì • ì—¬ë¶€ íŒë‹¨, íŒë‹¨ ê·¼ê±°ë¥¼ ìš”ì•½í•˜ì‹œì˜¤.\n\nì…ë ¥:\n{clause_text}"

    chat = [
        {"role": "system", "content": system},
        {"role": "user", "content": user}
    ]
    prompt = tok.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)
    inputs = tok(prompt, return_tensors="pt").to(mdl.device)
    out_ids = mdl.generate(
        **inputs, 
        max_new_tokens=256, 
        do_sample=False
    )
    out_txt = tok.decode(out_ids[0], skip_special_tokens=True)
    # ë§ˆì§€ë§‰ assistantë§Œ ì¶”ì¶œ
    ans = out_txt.split("assistant\n")[-1].strip()
    return ans

def parse_reason(answer_line: str) -> str:
    # "ë¶„ì•¼: X / ë¶ˆê³µì •ì—¬ë¶€: ìœ ë¦¬|ë¶ˆë¦¬ / ê·¼ê±°: Y" ì—ì„œ ê·¼ê±°ë§Œ ì¶”ì¶œ
    parts = [p.strip() for p in answer_line.split("/") if p.strip()]
    reason = ""
    for p in parts:
        if p.startswith("ê·¼ê±°:"):
            reason = p.replace("ê·¼ê±°:", "").strip()
            break
    return reason

# ==== 3) ê²€ìƒ‰ ====
def embed(embedder, texts):
    embs = embedder.encode(texts, normalize_embeddings=True)
    return np.asarray(embs, dtype="float32")

def search(index, query_vec, topk=5):
    D, I = index.search(query_vec, topk)
    return I[0], D[0]

# ==== 4) ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„± ====
def build_report(clause_text, sft_answer, meta, hits=None):
    answer_str = sft_answer.strip()
    is_unfair = "ë¶ˆê³µì •ì—¬ë¶€: ë¶ˆë¦¬" in answer_str

    report = {
        "input_clause": clause_text,
        "llm_output": answer_str,
    }

    laws = []
    if is_unfair and hits is not None and len(hits) > 0:
        for idx in hits:
            rec = meta[int(idx)]
            laws.append({
                "clauseField": rec.get("clauseField"),
                "law_text": rec.get("law_text")
            })
    report["retrieved_laws"] = laws
    return report



def main():
    index, meta = load_index_and_meta()
    tok, mdl = load_sft_model()
    embedder = load_embedder()

    print("âœ… ëª¨ë¸ ë° ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ.")
    print("ì—”í„°ë§Œ ëˆ„ë¥´ë©´ ì¢…ë£Œë©ë‹ˆë‹¤.\n")

    while True:
        clause = input("ğŸ” ì•½ê´€ ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”:\n> ").strip()
        if not clause:
            print("\nğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # 1) LLM ì¶”ë¡ 
        answer = run_sft(tok, mdl, clause)
        reason = parse_reason(answer)

        # 2) ìœ ë¦¬í•œ ê²½ìš° ê²€ìƒ‰ ìŠ¤í‚µ
        if "ë¶ˆê³µì •ì—¬ë¶€: ìœ ë¦¬" in answer:
            report = build_report(clause, answer, meta, hits=None)
        else:
            # ë¶ˆë¦¬í•œ ê²½ìš°ë§Œ ê·¼ê±° + ì›ë¬¸ìœ¼ë¡œ ê²€ìƒ‰
            fused_query = f"{clause}\n\níŒë‹¨ê·¼ê±°: {reason}" if reason else clause
            qv = embed(embedder, [fused_query])
            ids, _ = search(index, qv, topk=5)
            report = build_report(clause, answer, meta, hits=ids)

        print(json.dumps(report, ensure_ascii=False, indent=2))
        print("\n" + "="*80 + "\n")

if __name__ == "__main__":
    main()
