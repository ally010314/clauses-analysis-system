import torch, os, json, pandas as pd
from datasets import Dataset
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTTrainer, SFTConfig

# ----------------------------------------------------
# 1. ê¸°ë³¸ ì„¤ì •
# ----------------------------------------------------
MY_TOKEN = os.environ.get("HF_TOKEN")            # ì‹¤ì œ í† í°
BASE_MODEL = "meta-llama/Meta-Llama-3.1-8B-Instruct"
MAX_SEQ_LEN = 512
OUTPUT_DIR = "models/llama31-8b-sft-fold10"

# torch compile ì™„ì „ ë¹„í™œì„±í™”
os.environ["TORCH_COMPILE_DISABLE"] = "1"

# ----------------------------------------------------
# 2. ëª¨ë¸ / í† í¬ë‚˜ì´ì € ë¡œë“œ
# ----------------------------------------------------
bnb_cfg = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,    # âœ… 3070ìš©
    bnb_4bit_use_double_quant=True,
)

print(f"'{BASE_MODEL}' ë¡œë“œ ì¤‘ ...")
tok = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True, token=MY_TOKEN)
if tok.pad_token is None:
    tok.pad_token = tok.eos_token

mdl = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    quantization_config=bnb_cfg,
    attn_implementation="sdpa",              # âœ… ë¹ ë¥¸ Attention
    device_map="auto",
    dtype=torch.float16,
    token=MY_TOKEN,
)
print("--- ëª¨ë¸ ë¡œë”© ì™„ë£Œ ---")

# ----------------------------------------------------
# 3. 4-bit + LoRA ì¤€ë¹„
# ----------------------------------------------------
mdl = prepare_model_for_kbit_training(mdl)

lora_cfg = LoraConfig(
    r=8, lora_alpha=16, lora_dropout=0.05,
    target_modules=["q_proj","k_proj","v_proj",
                    "o_proj","gate_proj","up_proj","down_proj"],
    bias="none", task_type="CAUSAL_LM",
)
mdl = get_peft_model(mdl, lora_cfg)

# ----------------------------------------------------
# 4. ë°ì´í„° ë¡œë“œ
# ----------------------------------------------------
TRAIN_PATH = "data/kfold_data/train_fold_10.jsonl"
VAL_PATH   = "data/kfold_data/val_fold_10.jsonl"

def load_jsonl(path):
    with open(path, encoding="utf-8") as f:
        return [json.loads(l) for l in f if l.strip()]

train_rows = load_jsonl(TRAIN_PATH)
val_rows = load_jsonl(VAL_PATH)

train_df = pd.DataFrame(train_rows)
val_df = pd.DataFrame(val_rows)

# âœ… í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë§ˆì§€ë§‰ 100ê°œë§Œ ì‚¬ìš© (í•„ìš” ì‹œ ì œê±°)
#train_df = train_df.iloc[-100:].reset_index(drop=True)
print(f"Train ìƒ˜í”Œ ìˆ˜: {len(train_df)}, Val ìƒ˜í”Œ ìˆ˜: {len(val_df)}")

SYSTEM_PROMPT = (
    "ë‹¹ì‹ ì€ ì•½ê´€ì˜ ê³µì •ì„±ì„ ë¶„ì„í•˜ëŠ” ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n"
    "ë¬¸ë§¥ìƒ ì£¼ì²´ (ê³ ê°/ ì‚¬ì—…ì) ë¥¼ ëª…í™•íˆ êµ¬ë¶„í•˜ì„¸ìš”.\n"
    "ë°˜ë“œì‹œ ì•„ë˜ í•œ ì¤„ í¬ë§·ë§Œ ì¶œë ¥í•˜ì„¸ìš”:\n"
    "ë¶„ì•¼: <ì •ìˆ˜> / ë¶ˆê³µì •ì—¬ë¶€: <ìœ ë¦¬|ë¶ˆë¦¬> / ê·¼ê±°: <ê°„ê²°í•œ ë¬¸ì¥ ë˜ëŠ” 'í•´ë‹¹ ì—†ìŒ'>"
)

def to_messages(r):
    inst, inp, out = r.get("instruction",""), r.get("input",""), r.get("output","")
    user_text = inst if not inp else f"{inst}\n\nì…ë ¥:\n{inp}"
    return [
        {"role":"system","content":SYSTEM_PROMPT},
        {"role":"user","content":user_text},
        {"role":"assistant","content":out.strip()},
    ]
def format_example(ex):
    text = tok.apply_chat_template(
        to_messages(ex), tokenize=False, add_generation_prompt=False
    )
    return {"text": text}

# ----------------------------------------------------
# ë°ì´í„°ì…‹ ë³€í™˜ (ë²„ê·¸ ìˆ˜ì •)
# ----------------------------------------------------
train_ds = Dataset.from_pandas(train_df)
train_ds = train_ds.map(format_example, remove_columns=list(train_df.columns))

val_ds = Dataset.from_pandas(val_df)
val_ds = val_ds.map(format_example, remove_columns=list(val_df.columns))


print(f"ë°ì´í„°ì…‹: train {len(train_ds)}, val {len(val_ds)}")

# ----------------------------------------------------
# 5. í•™ìŠµ ì„¤ì •
# ----------------------------------------------------
sft_cfg = SFTConfig(
    output_dir=OUTPUT_DIR,
    num_train_epochs=1,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=4,
    learning_rate=2e-4,
    warmup_ratio=0.03,
    lr_scheduler_type="cosine",
    logging_strategy="steps",
    logging_steps=10,
    eval_strategy="epoch",             # âœ… í‰ê°€ ê°„ì†Œí™”
    save_strategy="epoch",             # âœ… ì €ì¥ ê°„ì†Œí™”
    save_total_limit=1,
    max_grad_norm=0.3,
    gradient_checkpointing=True,
    report_to="none",
    fp16=True, bf16=False,
    dataloader_num_workers=0,          # âœ… WSL ì•ˆì •í™”
    dataloader_pin_memory=False,
    dataset_text_field="text",
    max_length=MAX_SEQ_LEN,
    packing=False,
    group_by_length=True,               # âœ… ì†ë„/ì•ˆì • â†‘
    seed=42,
)

# ----------------------------------------------------
# 6. Trainer ì‹¤í–‰
# ----------------------------------------------------
trainer = SFTTrainer(
    model=mdl,
    train_dataset=train_ds,
    eval_dataset=val_ds,
    args=sft_cfg,
    processing_class=tok,
)

print("--- ğŸš€ íŒŒì¸íŠœë‹ ì‹œì‘ ---")
trainer.train()
print("--- ğŸ íŒŒì¸íŠœë‹ ì™„ë£Œ ---")

trainer.save_model(OUTPUT_DIR)
tok.save_pretrained(OUTPUT_DIR)
print(f"âœ… ì €ì¥ ì™„ë£Œ: {OUTPUT_DIR}")
