{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a834cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ally010314/llm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Train Dataset]\n",
      "ğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘: data/retriever_train.jsonl\n",
      " - ë°ì´í„° ê°œìˆ˜: 2430\n",
      "\n",
      "[Val Dataset]\n",
      "ğŸ“‚ ë°ì´í„° ë¡œë“œ ì¤‘: data/retriever_val.jsonl\n",
      " - ë°ì´í„° ê°œìˆ˜: 270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”¥ í•™ìŠµ ì‹œì‘...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='152' max='152' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [152/152 1:10:18, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.483800</td>\n",
       "      <td>0.368109</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ: models/kure-law-retriever-final\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datasets import Dataset \n",
    "from sentence_transformers import (\n",
    "    SentenceTransformer,\n",
    "    losses,\n",
    "    SentenceTransformerTrainer,\n",
    "    SentenceTransformerTrainingArguments\n",
    ")\n",
    "\n",
    "ROOT = Path(\".\")\n",
    "\n",
    "TRAIN_DATA_PATH = ROOT / \"data\" / \"retriever_train.jsonl\"\n",
    "VAL_DATA_PATH   = ROOT / \"data\" / \"retriever_val.jsonl\"\n",
    "\n",
    "LAW_DB_PATH = ROOT / \"data\" / \"law_db.json\"\n",
    "OUTPUT_DIR  = ROOT / \"models\" / \"kure-law-retriever\"\n",
    "\n",
    "def load_law_db():\n",
    "    if not LAW_DB_PATH.exists():\n",
    "        raise FileNotFoundError(f\"ë²•ë ¹ DB íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {LAW_DB_PATH}\")\n",
    "    with LAW_DB_PATH.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        law_list = json.load(f)\n",
    "    return { int(r[\"id\"]): r[\"law_text\"] for r in law_list }\n",
    "\n",
    "def build_dataset_from_file(file_path, law_db):\n",
    "    anchors = []\n",
    "    positives = []\n",
    "    \n",
    "    print(f\"ë°ì´í„° ë¡œë“œ ì¤‘: {file_path}\")\n",
    "    if not file_path.exists():\n",
    "        raise FileNotFoundError(f\"íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n",
    "\n",
    "    with file_path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line: continue\n",
    "            \n",
    "            ex = json.loads(line)\n",
    "            clause = ex.get(\"input\", \"\")\n",
    "            gt_ids = ex.get(\"ground_truth_ids\", [])\n",
    "            \n",
    "            if not clause or not gt_ids: continue\n",
    "            \n",
    "            # ì •ë‹µ ì¤‘ í•˜ë‚˜ ëœë¤ ì„ íƒ\n",
    "            pos_id = random.choice(gt_ids)\n",
    "            pos_text = law_db.get(int(pos_id))\n",
    "            \n",
    "            if not pos_text: continue\n",
    "            \n",
    "            anchors.append(clause)\n",
    "            positives.append(pos_text)\n",
    "\n",
    "    print(f\" - ë°ì´í„° ê°œìˆ˜: {len(anchors)}\")\n",
    "    \n",
    "    # HuggingFace Dataset ê°ì²´ ë°˜í™˜\n",
    "    return Dataset.from_dict({\n",
    "        \"anchor\": anchors,\n",
    "        \"positive\": positives\n",
    "    })\n",
    "\n",
    "def main():\n",
    "    #  ë²•ë ¹ DB ë¡œë“œ\n",
    "    id2text = load_law_db()\n",
    "    \n",
    "    #  Train / Val ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "    print(\"\\n[Train Dataset]\")\n",
    "    train_dataset = build_dataset_from_file(TRAIN_DATA_PATH, id2text)\n",
    "    \n",
    "    print(\"\\n[Val Dataset]\")\n",
    "    eval_dataset = build_dataset_from_file(VAL_DATA_PATH, id2text)\n",
    "\n",
    "    #  ëª¨ë¸ ë¡œë“œ\n",
    "    model = SentenceTransformer(\"nlpai-lab/KURE-v1\")\n",
    "\n",
    "    train_loss = losses.MultipleNegativesRankingLoss(model)\n",
    "\n",
    "    args = SentenceTransformerTrainingArguments(\n",
    "        output_dir=str(OUTPUT_DIR),\n",
    "        num_train_epochs=1,             \n",
    "        per_device_train_batch_size=2,  \n",
    "        per_device_eval_batch_size=2,  \n",
    "        gradient_accumulation_steps=8, \n",
    "        learning_rate=2e-5,            \n",
    "        warmup_steps=100,               \n",
    "        fp16=True,                   \n",
    "        logging_steps=10,             \n",
    "        save_strategy=\"epoch\",          \n",
    "        \n",
    "        eval_strategy=\"epoch\",          \n",
    "    )\n",
    "\n",
    "    #  Trainer ì‹¤í–‰\n",
    "    trainer = SentenceTransformerTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,      \n",
    "        loss=train_loss,\n",
    "    )\n",
    "\n",
    "    print(\"\\ní•™ìŠµ ì‹œì‘...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    # ì €ì¥\n",
    "    model.save_pretrained(str(OUTPUT_DIR))\n",
    "    print(f\"\\ní•™ìŠµ ë° ì €ì¥ ì™„ë£Œ: {OUTPUT_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d560ee83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Data] unique rows: 38\n",
      "[Embedding] loading: nlpai-lab/KURE-v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:03<00:00,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vectors=38, dim=1024\n",
      "- FAISS : rag_index/faiss.index\n",
      "- META  : rag_index/meta.pkl\n",
      "- DIM   : rag_index/dim.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import faiss\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "JSON_PATH = \"data/law_db.json\" \n",
    "INDEX_DIR = \"rag_index\"\n",
    "EMBEDDING_MODEL = os.environ.get(\"EMB_MODEL\", \"nlpai-lab/KURE-v1\")\n",
    "#EMBEDDING_MODEL = \"models/kure-law-retriever\"\n",
    "\n",
    "BATCH = 128\n",
    "\n",
    "\n",
    "def get_model(name: str):\n",
    "    print(f\"[Embedding] loading: {name}\")\n",
    "    model = SentenceTransformer(name)\n",
    "    return model\n",
    "\n",
    "\n",
    "def e5_encode_query(text: str) -> str:\n",
    "    return \"passage: \" + text.strip()\n",
    "\n",
    "\n",
    "def main():\n",
    "    os.makedirs(INDEX_DIR, exist_ok=True)\n",
    "    df = pd.read_json(JSON_PATH)\n",
    "    assert \"law_text\" in df.columns, \"JSONì— 'law_text' í•„ë“œê°€ í•„ìš”í•©ë‹ˆë‹¤.\"\n",
    "\n",
    "    df = df.drop_duplicates(subset=[\"law_text\"]).reset_index(drop=True)\n",
    "    print(f\"[Data] unique rows: {len(df)}\")\n",
    "\n",
    "    model = get_model(EMBEDDING_MODEL)\n",
    "    texts = [e5_encode_query(t) for t in df[\"law_text\"].tolist()]\n",
    "\n",
    "    # ë°°ì¹˜ ì„ë² ë”©\n",
    "    all_vecs = []\n",
    "    for i in tqdm(range(0, len(texts), BATCH), desc=\"Embedding\"):\n",
    "        batch = texts[i:i + BATCH]\n",
    "        vecs = model.encode(batch, normalize_embeddings=True, convert_to_numpy=True)\n",
    "        all_vecs.append(vecs)\n",
    "\n",
    "    X = np.vstack(all_vecs).astype(\"float32\")\n",
    "    dim = X.shape[1]\n",
    "\n",
    "    # FAISS Index\n",
    "    index = faiss.IndexFlatIP(dim)\n",
    "    index.add(X)\n",
    "    faiss.write_index(index, os.path.join(INDEX_DIR, \"faiss.index\"))\n",
    "\n",
    "    #ë©”íƒ€ë°ì´í„° ì €ì¥ (id, law_textë§Œ ìœ ì§€)\n",
    "    meta = []\n",
    "    for _, row in df.iterrows():\n",
    "        meta.append({\n",
    "            \"id\": int(row[\"id\"]),\n",
    "            \"file_name\": None,        \n",
    "            \"clauseField\": None,\n",
    "            \"law_text\": row[\"law_text\"],\n",
    "        })\n",
    "\n",
    "    with open(os.path.join(INDEX_DIR, \"meta.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(meta, f)\n",
    "\n",
    "    with open(os.path.join(INDEX_DIR, \"dim.txt\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(str(dim))\n",
    "\n",
    "    print(f\"vectors={len(df)}, dim={dim}\")\n",
    "    print(f\"- FAISS : {os.path.join(INDEX_DIR, 'faiss.index')}\")\n",
    "    print(f\"- META  : {os.path.join(INDEX_DIR, 'meta.pkl')}\")\n",
    "    print(f\"- DIM   : {os.path.join(INDEX_DIR, 'dim.txt')}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4661cfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base ëª¨ë¸ ë¡œë“œ ì¤‘: nlpai-lab/KURE-v1\n",
      "ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘: rag_index/faiss.index\n",
      "í‰ê°€ ë°ì´í„° ë¡œë“œ: data/retriever_val.jsonl\n",
      "\n",
      "[Case 1: Base Model] í‰ê°€ ì‹œì‘ (ì´ 270ê°œ)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 270/270 [00:17<00:00, 15.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "[Case 1 ê²°ê³¼] KURE-v1 Base Model\n",
      "========================================\n",
      "Recall@1  : 0.0889\n",
      "Recall@3  : 0.2111\n",
      "Recall@5  : 0.2926\n",
      "Recall@10 : 0.4556\n",
      "MRR       : 0.1847\n",
      "nDCG@5    : 0.1964\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import faiss\n",
    "import math\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "ROOT = Path(\".\")\n",
    "DATA_PATH = ROOT / \"data\" / \"retriever_val.jsonl\"\n",
    "\n",
    "BASE_MODEL_NAME = \"nlpai-lab/KURE-v1\"\n",
    "\n",
    "INDEX_PATH = ROOT / \"rag_index\" / \"faiss.index\"\n",
    "META_PATH  = ROOT / \"rag_index\" / \"meta.pkl\"\n",
    "\n",
    "def load_resources():\n",
    "    print(f\"Base ëª¨ë¸ ë¡œë“œ ì¤‘: {BASE_MODEL_NAME}\")\n",
    "    embedder = SentenceTransformer(BASE_MODEL_NAME)\n",
    "    \n",
    "    print(f\"ì¸ë±ìŠ¤ ë¡œë“œ ì¤‘: {INDEX_PATH}\")\n",
    "    if not INDEX_PATH.exists():\n",
    "        raise FileNotFoundError(f\"ì¸ë±ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {INDEX_PATH}\")\n",
    "        \n",
    "    index = faiss.read_index(str(INDEX_PATH))\n",
    "    \n",
    "    with open(META_PATH, \"rb\") as f:\n",
    "        meta = pickle.load(f)\n",
    "        \n",
    "    return embedder, index, meta\n",
    "\n",
    "def calculate_metrics(hits, gt_ids):\n",
    "    # Recall\n",
    "    r1  = 1.0 if any(h in gt_ids for h in hits[:1]) else 0.0\n",
    "    r3  = 1.0 if any(h in gt_ids for h in hits[:3]) else 0.0\n",
    "    r5  = 1.0 if any(h in gt_ids for h in hits[:5]) else 0.0\n",
    "    r10 = 1.0 if any(h in gt_ids for h in hits[:10]) else 0.0\n",
    "    \n",
    "    # MRR\n",
    "    mrr = 0.0\n",
    "    for rank, h in enumerate(hits, 1):\n",
    "        if h in gt_ids:\n",
    "            mrr = 1.0 / rank\n",
    "            break\n",
    "            \n",
    "    # nDCG@5\n",
    "    dcg = 0.0\n",
    "    for rank, h in enumerate(hits[:5], 1):\n",
    "        if h in gt_ids:\n",
    "            dcg = 1.0 / math.log2(rank + 1)\n",
    "            break\n",
    "            \n",
    "    return r1, r3, r5, r10, mrr, dcg\n",
    "\n",
    "def main():\n",
    "    # 1. ë¦¬ì†ŒìŠ¤ ë¡œë“œ\n",
    "    embedder, index, meta = load_resources()\n",
    "    \n",
    "    # 2. ë°ì´í„° ë¡œë“œ\n",
    "    print(f\"í‰ê°€ ë°ì´í„° ë¡œë“œ: {DATA_PATH}\")\n",
    "    with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = [json.loads(line) for line in f if line.strip()]\n",
    "        \n",
    "    print(f\"\\n[Case 1: Base Model] í‰ê°€ ì‹œì‘ (ì´ {len(data)}ê°œ)...\")\n",
    "    \n",
    "    stats = {\"r1\": 0.0, \"r3\": 0.0, \"r5\": 0.0, \"r10\": 0.0, \"mrr\": 0.0, \"ndcg5\": 0.0}\n",
    "    cnt = 0\n",
    "    \n",
    "    for ex in tqdm(data):\n",
    "        clause = ex.get(\"input\", \"\")\n",
    "        gt_ids = set(ex.get(\"ground_truth_ids\", []))\n",
    "        \n",
    "        if not gt_ids: continue\n",
    "        \n",
    "        query = clause\n",
    "        \n",
    "        vec = embedder.encode([query], normalize_embeddings=True)\n",
    "        _, I = index.search(vec.astype(\"float32\"), 10)\n",
    "        \n",
    "        pred_ids = []\n",
    "        for idx in I[0]:\n",
    "            if idx == -1: continue\n",
    "            pred_ids.append(meta[int(idx)][\"id\"])\n",
    "            \n",
    "        r1, r3, r5, r10, mrr, ndcg = calculate_metrics(pred_ids, gt_ids)\n",
    "        \n",
    "        stats[\"r1\"] += r1\n",
    "        stats[\"r3\"] += r3\n",
    "        stats[\"r5\"] += r5\n",
    "        stats[\"r10\"] += r10\n",
    "        stats[\"mrr\"] += mrr\n",
    "        stats[\"ndcg5\"] += ndcg\n",
    "        cnt += 1\n",
    "        \n",
    "    if cnt == 0:\n",
    "        print(\"í‰ê°€í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "        return\n",
    "\n",
    "    for k in stats: stats[k] /= cnt\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(f\"[Case 1 ê²°ê³¼] KURE-v1 Base Model\")\n",
    "    print(\"=\"*40)\n",
    "    print(f\"Recall@1  : {stats['r1']:.4f}\")\n",
    "    print(f\"Recall@3  : {stats['r3']:.4f}\")\n",
    "    print(f\"Recall@5  : {stats['r5']:.4f}\")\n",
    "    print(f\"Recall@10 : {stats['r10']:.4f}\")\n",
    "    print(f\"MRR       : {stats['mrr']:.4f}\")\n",
    "    print(f\"nDCG@5    : {stats['ndcg5']:.4f}\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "249c9834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fine-tuned ëª¨ë¸ ë¡œë“œ: models/kure-law-retriever\n",
      " FAISS ì¸ë±ìŠ¤ ë¡œë“œ: rag_index_retriever/faiss.index\n",
      " ë°ì´í„° ë¡œë“œ: data/retriever_val.jsonl\n",
      "\n",
      " [Case 2: Only Clause (Fine-tuned)] í‰ê°€ ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 270/270 [04:08<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " [Case 3: Clause + Reason (Fine-tuned)] í‰ê°€ ì¤‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 270/270 [07:46<00:00,  1.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Metric     | Case 2 (Only Clause)   | Case 3 (+Reason)      \n",
      "------------------------------------------------------------\n",
      "r1         | 0.3778                 | 0.3926 (+0.0148)\n",
      "r3         | 0.6185                 | 0.6148 (-0.0037)\n",
      "r5         | 0.6926                 | 0.7148 (+0.0222)\n",
      "r10        | 0.8370                 | 0.8519 (+0.0148)\n",
      "mrr        | 0.5235                 | 0.5332 (+0.0097)\n",
      "ndcg5      | 0.5519                 | 0.5647 (+0.0128)\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import faiss\n",
    "import math\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "ROOT = Path(\".\")\n",
    "DATA_PATH = ROOT / \"data\" / \"retriever_val.jsonl\" \n",
    "\n",
    "\n",
    "MODEL_PATH = ROOT / \"models\" / \"kure-law-retriever\"\n",
    "INDEX_PATH = ROOT / \"rag_index_retriever\" / \"faiss.index\" \n",
    "META_PATH  = ROOT / \"rag_index_retriever\" / \"meta.pkl\"    \n",
    "\n",
    "def load_resources():\n",
    "    print(f\" Fine-tuned ëª¨ë¸ ë¡œë“œ: {MODEL_PATH}\")\n",
    "    embedder = SentenceTransformer(str(MODEL_PATH))\n",
    "    \n",
    "    print(f\" FAISS ì¸ë±ìŠ¤ ë¡œë“œ: {INDEX_PATH}\")\n",
    "    if not INDEX_PATH.exists():\n",
    "        raise FileNotFoundError(f\"ì¸ë±ìŠ¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {INDEX_PATH}\")\n",
    "        \n",
    "    index = faiss.read_index(str(INDEX_PATH))\n",
    "    with open(META_PATH, \"rb\") as f:\n",
    "        meta = pickle.load(f)\n",
    "    return embedder, index, meta\n",
    "\n",
    "def parse_reason_from_answer(answer_line: str) -> str:\n",
    "    \"\"\"\n",
    "    sft_answer ë¬¸ìì—´: \"ë¶„ì•¼: 3 / ë¶ˆê³µì •ì—¬ë¶€: ë¶ˆë¦¬ / ê·¼ê±°: ......\"\n",
    "    ì—¬ê¸°ì„œ 'ê·¼ê±°: ' ë’¤ì˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•¨.\n",
    "    \"\"\"\n",
    "    if not answer_line:\n",
    "        return \"\"\n",
    "        \n",
    "    parts = [p.strip() for p in answer_line.split(\"/\") if p.strip()]\n",
    "    \n",
    "    for p in parts:\n",
    "        if p.startswith(\"ê·¼ê±°:\"):\n",
    "            return p.replace(\"ê·¼ê±°:\", \"\").strip()\n",
    "            \n",
    "    return \"\"\n",
    "\n",
    "def calculate_metrics(hits, gt_ids):\n",
    "    # Recall\n",
    "    r1  = 1.0 if any(h in gt_ids for h in hits[:1]) else 0.0\n",
    "    r3  = 1.0 if any(h in gt_ids for h in hits[:3]) else 0.0\n",
    "    r5  = 1.0 if any(h in gt_ids for h in hits[:5]) else 0.0\n",
    "    r10 = 1.0 if any(h in gt_ids for h in hits[:10]) else 0.0\n",
    "    \n",
    "    # MRR\n",
    "    mrr = 0.0\n",
    "    for rank, h in enumerate(hits, 1):\n",
    "        if h in gt_ids:\n",
    "            mrr = 1.0 / rank\n",
    "            break\n",
    "    # nDCG@5\n",
    "    dcg = 0.0\n",
    "    for rank, h in enumerate(hits[:5], 1):\n",
    "        if h in gt_ids:\n",
    "            dcg = 1.0 / math.log2(rank + 1)\n",
    "            break\n",
    "    return r1, r3, r5, r10, mrr, dcg \n",
    "\n",
    "def evaluate(name, data, embedder, index, meta, mode):\n",
    "    print(f\"\\n [{name}] í‰ê°€ ì¤‘...\")\n",
    "    stats = {\"r1\": 0.0, \"r3\": 0.0, \"r5\": 0.0, \"r10\": 0.0, \"mrr\": 0.0, \"ndcg5\": 0.0}\n",
    "    cnt = 0\n",
    "    \n",
    "    for ex in tqdm(data):\n",
    "        clause = ex.get(\"input\", \"\")\n",
    "        gt_ids = set(ex.get(\"ground_truth_ids\", []))\n",
    "        if not gt_ids: continue\n",
    "        \n",
    "        if mode == \"reason\": \n",
    "            sft_ans = ex.get(\"sft_answer\", \"\")\n",
    "            \n",
    "            reason = parse_reason_from_answer(sft_ans)\n",
    "            \n",
    "            if reason and reason != \"í•´ë‹¹ ì—†ìŒ\":\n",
    "                query = f\"{clause}\\n\\níŒë‹¨ê·¼ê±°: {reason}\"\n",
    "            else:\n",
    "                query = clause\n",
    "        else:\n",
    "            query = clause\n",
    "            \n",
    "        vec = embedder.encode([query], normalize_embeddings=True)\n",
    "        _, I = index.search(vec.astype(\"float32\"), 10)\n",
    "        \n",
    "        pred_ids = []\n",
    "        for idx in I[0]:\n",
    "            if idx == -1: continue\n",
    "            pred_ids.append(meta[int(idx)][\"id\"])\n",
    "            \n",
    "        r1, r3, r5, r10, mrr, ndcg = calculate_metrics(pred_ids, gt_ids)\n",
    "        stats[\"r1\"] += r1\n",
    "        stats[\"r3\"] += r3\n",
    "        stats[\"r5\"] += r5\n",
    "        stats[\"r10\"] += r10\n",
    "        stats[\"mrr\"] += mrr\n",
    "        stats[\"ndcg5\"] += ndcg\n",
    "        cnt += 1\n",
    "        \n",
    "    for k in stats: stats[k] /= cnt\n",
    "    return stats\n",
    "\n",
    "def main():\n",
    "    embedder, index, meta = load_resources()\n",
    "    \n",
    "    print(f\" ë°ì´í„° ë¡œë“œ: {DATA_PATH}\")\n",
    "    with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = [json.loads(line) for line in f if line.strip()]\n",
    "        \n",
    "    res_case2 = evaluate(\"Case 2: Only Clause (Fine-tuned)\", data, embedder, index, meta, mode=\"clause\")\n",
    "    \n",
    "    res_case1 = evaluate(\"Case 3: Clause + Reason (Fine-tuned)\", data, embedder, index, meta, mode=\"reason\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"{'Metric':<10} | {'Case 2 (Only Clause)':<22} | {'Case 3 (+Reason)':<22}\")\n",
    "    print(\"-\" * 60)\n",
    "    for k in [\"r1\", \"r3\", \"r5\", \"r10\", \"mrr\", \"ndcg5\"]:\n",
    "        diff = res_case1[k] - res_case2[k]\n",
    "        print(f\"{k:<10} | {res_case2[k]:.4f}                 | {res_case1[k]:.4f} ({diff:+.4f})\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llm)",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
